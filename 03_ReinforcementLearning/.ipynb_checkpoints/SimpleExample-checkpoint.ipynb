{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ce89ac1-baa4-45f7-9ace-241b43d3ffd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e90412b5-ffe9-4225-a668-ba135b600528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the maze (1 represents walls, 0 represents empty space)\n",
    "maze = np.array([\n",
    "    [1, 1, 1, 1, 1],\n",
    "    [1, 0, 0, 0, 1],\n",
    "    [1, 0, 1, 0, 1],\n",
    "    [1, 0, 0, 0, 1],\n",
    "    [1, 1, 1, 1, 1]\n",
    "])\n",
    "\n",
    "# Define the starting position and the goal position\n",
    "start = (1,1)\n",
    "goal = (3,3)\n",
    "\n",
    "# Define the action the robot can take (up, down, left, right)\n",
    "actions = [(0,1), (0,-1), (1,0), (-1,0)]\n",
    "\n",
    "# Initialize the Q-table with zeros\n",
    "q_table = np.zeros((maze.shape[0], maze.shape[1], len(actions)))\n",
    "# print(q_table)\n",
    "\n",
    "# Define parameters\n",
    "learning_rate = 0.1\n",
    "discount_factor = 0.9\n",
    "exploration_rate = 0.1\n",
    "num_episodes = 1000\n",
    "\n",
    "# Define the epsilon-greedy policy\n",
    "def choose_action(state):\n",
    "    if np.random.uniform(0,1) < exploration_rate:\n",
    "        return np.random.choice(len(actions))\n",
    "    else:\n",
    "        return np.argmax(q_table[state])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cb21a6ed-dc28-4cbe-982b-792a69d780e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current state is : (1, 1)\n",
      "next state is : (1, 2)\n",
      "current state is : (1, 2)\n",
      "next state is : (1, 1)\n",
      "current state is : (1, 1)\n",
      "next state is : (2, 1)\n",
      "current state is : (2, 1)\n",
      "next state is : (1, 1)\n",
      "current state is : (1, 1)\n",
      "next state is : (1, 2)\n",
      "current state is : (1, 2)\n",
      "next state is : (2, 2)\n",
      "current state is : (2, 2)\n",
      "next state is : (3, 2)\n",
      "current state is : (3, 2)\n",
      "next state is : (3, 1)\n",
      "current state is : (3, 1)\n",
      "next state is : (3, 0)\n",
      "current state is : (3, 0)\n",
      "next state is : (3, 1)\n",
      "current state is : (3, 1)\n",
      "next state is : (4, 1)\n",
      "current state is : (4, 1)\n",
      "next state is : (4, 2)\n",
      "current state is : (4, 2)\n",
      "next state is : (4, 3)\n",
      "current state is : (4, 3)\n",
      "next state is : (4, 4)\n",
      "current state is : (4, 4)\n",
      "next state is : (4, 5)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 5 is out of bounds for axis 1 with size 5",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcurrent state is : \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(state))\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnext state is : \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(next_state))\n\u001b[0;32m----> 9\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mmaze\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5\u001b[39m  \u001b[38;5;66;03m# Punish hitting walls\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Update Q-table\u001b[39;00m\n\u001b[1;32m     12\u001b[0m q_table[state][action] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m learning_rate \u001b[38;5;241m*\u001b[39m (reward \u001b[38;5;241m+\u001b[39m \n\u001b[1;32m     13\u001b[0m                     discount_factor \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(q_table[next_state]) \u001b[38;5;241m-\u001b[39m q_table[state][action])\n",
      "\u001b[0;31mIndexError\u001b[0m: index 5 is out of bounds for axis 1 with size 5"
     ]
    }
   ],
   "source": [
    "# Run the episodes\n",
    "for episode in range(num_episodes):\n",
    "    state = start\n",
    "    while state != goal:\n",
    "        action = choose_action(state)\n",
    "        next_state = (state[0] + actions[action][0], state[1] + actions[action][1])\n",
    "        print(\"current state is : \" + str(state))\n",
    "        print(\"next state is : \" + str(next_state))\n",
    "        reward = -1 if maze[next_state] == 0 else -5  # Punish hitting walls\n",
    "\n",
    "        # Update Q-table\n",
    "        q_table[state][action] += learning_rate * (reward + \n",
    "                            discount_factor * np.max(q_table[next_state]) - q_table[state][action])\n",
    "        \n",
    "        state = next_state\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421b20c4-c9e4-456f-a0d2-8ac664cc68c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
