{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc3ffecb-6215-4d49-8adb-07a6dc6d07a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import functools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "DEVICE = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f02c0f42-5832-4917-9636-6b550b184980",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(board, handle=None, sleep_sec=0.8):\n",
    "    \"\"\" display board \"\"\"\n",
    "    board = board.detach().cpu()\n",
    "    s_booard = np.zeros(board.shape, dtype='U')\n",
    "    s_booard[board == +1] = '○'\n",
    "    s_booard[board == -1] = '×'\n",
    "    df = pd.DataFrame(s_booard.squeeze())\n",
    "    if handle is None:\n",
    "        handle = display(df, display_id=True)\n",
    "    else:\n",
    "        handle.update(df)\n",
    "    time.sleep(sleep_sec)\n",
    "    return handle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cb2beee-0b83-4a7a-9070-168638e250f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter which inspect three stone conteniusly. shape is (4,1,3,3)\n",
    "FILTERS = torch.tensor([\n",
    "    # holizontal\n",
    "    [[[0,0,0],\n",
    "      [1,1,1],\n",
    "      [0,0,0]]],\n",
    "    # vertival\n",
    "    [[[0,1,0],\n",
    "      [0,1,0],\n",
    "      [0,1,0]]],\n",
    "    # to top left\n",
    "    [[[1,0,0],\n",
    "      [0,1,0],\n",
    "      [0,0,1]]],\n",
    "    # to top right\n",
    "    [[[0,0,1],\n",
    "      [0,1,0],\n",
    "      [1,0,0]]],\n",
    "], dtype=torch.float32, device=DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bde8a481-6c09-4f53-981f-1271699d3f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_move(board, player, nv, out=None):\n",
    "    \"\"\" judge win/lose of next move nv\n",
    "    Args\n",
    "      board : current board. shape is (1,1,3,3)\n",
    "      player : +1 or -1\n",
    "      nv :  flat coodinate of next move\n",
    "      out : board which store the next move. shape is (1,1,3,3)\n",
    "    Return\n",
    "      state : winner +1 or -1. if draw,  0\n",
    "    \"\"\"\n",
    "    assert board.numel() == 9\n",
    "    assert player in (-1, +1)\n",
    "    assert nv in range(board.numel())\n",
    "    if board.flatten()[nv] != 0:\n",
    "        return -player\n",
    "    out = board.detach().clone() if out is None else out\n",
    "    out.flatten()[nv] = player # inpout next move\n",
    "    n_match = F.conv2d(out.view(1,1,3,3), FILTERS, stride=1, padding=1) # count consecutive stones\n",
    "    mask = n_match.abs() == 3 # coodinate that stones are 3 consecutives\n",
    "    state = n_match[mask].sign().sum().clamp(-1,1) # winner\n",
    "    return state.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13b19a1f-b26c-42f3-90f1-a69329e70ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_play(board, policies, display_handle=None):\n",
    "    policies = np.broadcast_to(policies,2) # policies of players\n",
    "    board[:] = 0 # reset board\n",
    "    if display_handle:\n",
    "        plot(board, display_handle) # display board\n",
    "    player = torch.tensor(1).float()\n",
    "    for turn in range(board.numel()):\n",
    "        bin = int(player == -1)\n",
    "        nv = policies[bin](board, player) # select next move\n",
    "        state = next_move(board, player, nv, out=board) # hit next move\n",
    "        if display_handle:\n",
    "            plot(board, display_handle)\n",
    "        if state != 0: # finish if w/l has been decided\n",
    "            break\n",
    "        player = -player # change players\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6404e7a5-5346-4c52-8030-8da2d135347b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_random(board, player):\n",
    "    blanks, = torch.where(board.flatten()== 0)\n",
    "    nv = torch.randint(len(blanks), (1,))\n",
    "    return blanks[nv]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb50f653-0ab6-4e81-b4e1-fbeeb42ea684",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>○</td>\n",
       "      <td></td>\n",
       "      <td>×</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td>○</td>\n",
       "      <td>×</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>○</td>\n",
       "      <td>×</td>\n",
       "      <td>○</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1  2\n",
       "0  ○     ×\n",
       "1     ○  ×\n",
       "2  ○  ×  ○"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'○ wins'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run():\n",
    "    handle = display(None, display_id=True)\n",
    "    board = torch.zeros((3,3), dtype=torch.float32, device=DEVICE)\n",
    "    state = auto_play(board, policy_random, display_handle=handle)\n",
    "    msg = {-1: '× wins', 0: 'draw', 1: '○ wins'}\n",
    "    return msg[int(state)]\n",
    "\n",
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1554c3e4-3c5d-4ef2-904e-86bb112fa95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_play(board, policy, N=100):\n",
    "    \"\"\" repeat games N times\n",
    "    Return\n",
    "    counts : [count wins of later, count of draws, count wins of former]\n",
    "    \"\"\"\n",
    "    states = [int(auto_play(board, policy)) for i in range(N)]\n",
    "    u, c = np.unique(states, return_counts=True)\n",
    "    assert set(u).issubset({-1, 0, 1})\n",
    "    counts = np.zeros(3, 'i')\n",
    "    counts [u+1] = c\n",
    "    return counts\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8dfc4aa9-d31c-409b-8057-943c2d81fc4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[later draw former]\n",
      "[0.29 0.12 0.59] = [293 119 588] / 1000\n"
     ]
    }
   ],
   "source": [
    "def run(N=100):\n",
    "    board = torch.zeros((3,3), dtype=torch.float32, device=DEVICE)\n",
    "    counts = repeat_play(board, policy_random, N=N)\n",
    "    with np.printoptions(precision=2, floatmode='fixed'):\n",
    "        print('[later draw former]')\n",
    "        print(counts / counts.sum(), '=', counts, '/', counts.sum())\n",
    "\n",
    "run(N=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "91bf2ac7-092e-41cf-bde0-91ec45c5cbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate(net, board, player):\n",
    "    \"\"\" AI estimate \"\"\"\n",
    "    board = player * board\n",
    "    board = board.view(1,1, *board.size())\n",
    "    return net(board).flatten()\n",
    "\n",
    "def policy_ai(board, player, net=None):\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        blanks, = torch.where(board.flatten() == 0)\n",
    "        Q = estimate(net, board, player)\n",
    "        actoin = blanks[Q[blanks].argmax()]\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8beeaa85-3134-452b-8926-8ec643279706",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_train(board, player, net=None, optimizer=None,epsilon=0.5,gamma=1., report=None):\n",
    "    \"\"\" train AI \"\"\"\n",
    "    ######################\n",
    "    # estimated number of action values\n",
    "    ######################\n",
    "    net.train()\n",
    "    optimizer.zero_grad()\n",
    "    Q = estimate(net, board, player) # estimate action value\n",
    "    blanks, = torch.where(board.flatten() == 0) # get blank cell\n",
    "    if torch.rand((1,)) < epsilon:\n",
    "        i = torch.randint(len(blanks), (1,)) # select random move from blanks\n",
    "        action = blanks[i.item()]\n",
    "    else:\n",
    "        action = blanks[Q[blanks].argmax()] # select next move from the maximum value from blanks\n",
    "\n",
    "    Q_action = Q[action] # value of next action\n",
    "\n",
    "    ######################\n",
    "    # target number of action values\n",
    "    ######################\n",
    "    net.eval()\n",
    "    with torch.nograd():\n",
    "        post = board.detach().clo0ne()\n",
    "        state = next_move(board, player, action, out=post) # judege win/lose of next move\n",
    "        blanks_next, = torch.where(post.flatten() == 0) # get blank cells of next move\n",
    "        reward, Q_next = 0, 0\n",
    "        if state == player: # player wins\n",
    "            reward = 1 \n",
    "        elif state == -player: # player loses\n",
    "            reward = -1\n",
    "        elif len(blanks_next) == 0: # draw\n",
    "            reward = 0\n",
    "        else:                       # win/lose is not decided\n",
    "            Q = estimate(net, post, -player) # estimate action value of next of next move \n",
    "            Q_next = Q[blanks_next].max()    # select maximum value of action\n",
    "\n",
    "        Q_target = reward - gamma * Q_next\n",
    "        Q_target = torch.as_tensor(Q_target, dtype=torch.float32, device=Q_action.device)\n",
    "\n",
    "    ######################\n",
    "    # update paramaters\n",
    "    ######################\n",
    "    net.train()\n",
    "    loss = F.mse_loss(Q_action, Q_target)\n",
    "    loss.backjward()\n",
    "    optimizer.step()\n",
    "    if report is not None:\n",
    "        report['loss'] += loss.item()\n",
    "\n",
    "    return action\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "31bd28d1-e9f4-4632-87cc-ab1ebce84693",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Conv2d.__init__() got an unexpected keyword argument 'karnel_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 58\u001b[0m\n\u001b[1;32m     54\u001b[0m             start_tm \u001b[38;5;241m=\u001b[39m current_tm\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cnn\n\u001b[0;32m---> 58\u001b[0m cnn \u001b[38;5;241m=\u001b[39m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_episode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[19], line 4\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(n_episode, interval, lr, N)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(n_episode\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m, interval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m, N\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m):\n\u001b[1;32m      2\u001b[0m     dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m128\u001b[39m\n\u001b[1;32m      3\u001b[0m     cnn \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[0;32m----> 4\u001b[0m         \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mConv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkarnel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m,\n\u001b[1;32m      5\u001b[0m         nn\u001b[38;5;241m.\u001b[39mFlatten(),\n\u001b[1;32m      6\u001b[0m         nn\u001b[38;5;241m.\u001b[39mLinear(dim, dim, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m      7\u001b[0m         nn\u001b[38;5;241m.\u001b[39mReLU(\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m      8\u001b[0m         nn\u001b[38;5;241m.\u001b[39mLinear(dim, dim, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m      9\u001b[0m         nn\u001b[38;5;241m.\u001b[39mReLU(\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m     10\u001b[0m         nn\u001b[38;5;241m.\u001b[39mLinear(dim, \u001b[38;5;241m3\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m3\u001b[39m, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m     11\u001b[0m         nn\u001b[38;5;241m.\u001b[39mTanh(),\n\u001b[1;32m     12\u001b[0m     )\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# policies while evaluation\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     ai_vs_random \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     16\u001b[0m         functools\u001b[38;5;241m.\u001b[39mpartial(policy_ai, net\u001b[38;5;241m=\u001b[39mcnn),\n\u001b[1;32m     17\u001b[0m         policy_random,\n\u001b[1;32m     18\u001b[0m     )\n",
      "\u001b[0;31mTypeError\u001b[0m: Conv2d.__init__() got an unexpected keyword argument 'karnel_size'"
     ]
    }
   ],
   "source": [
    "def run(n_episode=10000, interval=500, lr=0.01, N=100):\n",
    "    dim = 128\n",
    "    cnn = nn.Sequential(\n",
    "        nn.Conv2d(1, dim, kernel_size=3, padding=0, bias=False),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(dim, dim, bias=True),\n",
    "        nn.ReLU(True),\n",
    "        nn.Linear(dim, dim, bias=True),\n",
    "        nn.ReLU(True),\n",
    "        nn.Linear(dim, 3*3, bias=True),\n",
    "        nn.Tanh(),\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    # policies while evaluation\n",
    "    ai_vs_random = (\n",
    "        functools.partial(policy_ai, net=cnn),\n",
    "        policy_random,\n",
    "    )\n",
    "    random_vs_ai = (\n",
    "        policy_random,\n",
    "        functools.partial(policy_ai, net=cnn),\n",
    "    )\n",
    "\n",
    "    # policies while learning\n",
    "    op = torch.optim.SGD(cnn.parameters(), lr=lr)\n",
    "    rp = {'loss'* 0.}\n",
    "    policy = functools.partial(policy_train, net=cnn, optimizer=op, reporter=rp)\n",
    "    \n",
    "    print('#[later draw former]')\n",
    "    n_train = np.array([0,0,0])\n",
    "    board = torch.zeros((3,3), dtype=torch.float32, device=DEVICE)\n",
    "    start_tm = time.time()\n",
    "    for i in range(n_episode):\n",
    "        winner = int(auto_play(board, policy))\n",
    "        n_train[winner+1] += 1\n",
    "\n",
    "        if i==0 or (i+1) & interval == 0 or i+1 == n_episode:\n",
    "            n_1st = repeat_play(board, ai_vs_random, N=N)\n",
    "            n_2nd = repeat_play(board, random_vs_ai, N=N)\n",
    "            loss = rp['loss']\n",
    "            current_tm =  time.time()\n",
    "\n",
    "            with np.printoptions(formatter={'float': '{:02.0f}'.format}):\n",
    "                print('[{}/{}] loss:{:.3f} %Train:{} %1st:{} %2nd:{} {:.3f}s'.format(\n",
    "                    i+1, n_episode, loss,\n",
    "                    100 * n_train / n_train.sum(),\n",
    "                    100 * n_1st / n_1st.sum(),\n",
    "                    100 * n_2nd / n_2nd.sum(),\n",
    "                    current_tm - start_tm,\n",
    "                ))\n",
    "\n",
    "            n_train[:] = 0\n",
    "            rp['loss'] = 0\n",
    "            start_tm = current_tm\n",
    "\n",
    "    return cnn\n",
    "\n",
    "cnn = run(n_episode=10000, interval=500, lr=0.01, N=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
