{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc3ffecb-6215-4d49-8adb-07a6dc6d07a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import functools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "DEVICE = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f02c0f42-5832-4917-9636-6b550b184980",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(board, handle=None, sleep_sec=0.8):\n",
    "    \"\"\" display board \"\"\"\n",
    "    board = board.detach().cpu()\n",
    "    s_booard = np.zeros(board.shape, dtype='U')\n",
    "    s_booard[board == +1] = '○'\n",
    "    s_booard[board == -1] = '×'\n",
    "    df = pd.DataFrame(s_booard.squeeze())\n",
    "    if handle is None:\n",
    "        handle = display(df, display_id=True)\n",
    "    else:\n",
    "        handle.update(df)\n",
    "    time.sleep(sleep_sec)\n",
    "    return handle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cb2beee-0b83-4a7a-9070-168638e250f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter which inspect three stone conteniusly. shape is (4,1,3,3)\n",
    "FILTERS = torch.tensor([\n",
    "    # holizontal\n",
    "    [[[0,0,0],\n",
    "      [1,1,1],\n",
    "      [0,0,0]]],\n",
    "    # vertival\n",
    "    [[[0,1,0],\n",
    "      [0,1,0],\n",
    "      [0,1,0]]],\n",
    "    # to top left\n",
    "    [[[1,0,0],\n",
    "      [0,1,0],\n",
    "      [0,0,1]]],\n",
    "    # to top right\n",
    "    [[[0,0,1],\n",
    "      [0,1,0],\n",
    "      [1,0,0]]],\n",
    "], dtype=torch.float32, device=DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bde8a481-6c09-4f53-981f-1271699d3f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_move(board, player, nv, out=None):\n",
    "    \"\"\" judge win/lose of next move nv\n",
    "    Args\n",
    "      board : current board. shape is (1,1,3,3)\n",
    "      player : +1 or -1\n",
    "      nv :  flat coodinate of next move\n",
    "      out : board which store the next move. shape is (1,1,3,3)\n",
    "    Return\n",
    "      state : winner +1 or -1. if draw,  0\n",
    "    \"\"\"\n",
    "    assert board.numel() == 9\n",
    "    assert player in (-1, +1)\n",
    "    assert nv in range(board.numel())\n",
    "    if board.flatten()[nv] != 0:\n",
    "        return -player\n",
    "    out = board.detach().clone() if out is None else out\n",
    "    out.flatten()[nv] = player # inpout next move\n",
    "    n_match = F.conv2d(out.view(1,1,3,3), FILTERS, stride=1, padding=1) # count consecutive stones\n",
    "    mask = n_match.abs() == 3 # coodinate that stones are 3 consecutives\n",
    "    state = n_match[mask].sign().sum().clamp(-1,1) # winner\n",
    "    return state.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13b19a1f-b26c-42f3-90f1-a69329e70ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_play(board, policies, display_handle=None):\n",
    "    policies = np.broadcast_to(policies,2) # policies of players\n",
    "    board[:] = 0 # reset board\n",
    "    if display_handle:\n",
    "        plot(board, display_handle) # display board\n",
    "    player = torch.tensor(1).float()\n",
    "    for turn in range(board.numel()):\n",
    "        bin = int(player == -1)\n",
    "        nv = policies[bin](board, player) # select next move\n",
    "        state = next_move(board, player, nv, out=board) # hit next move\n",
    "        if display_handle:\n",
    "            plot(board, display_handle)\n",
    "        if state != 0: # finish if w/l has been decided\n",
    "            break\n",
    "        player = -player # change players\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6404e7a5-5346-4c52-8030-8da2d135347b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_random(board, player):\n",
    "    blanks, = torch.where(board.flatten()== 0)\n",
    "    nv = torch.randint(len(blanks), (1,))\n",
    "    return blanks[nv]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb50f653-0ab6-4e81-b4e1-fbeeb42ea684",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>○</td>\n",
       "      <td></td>\n",
       "      <td>×</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td>○</td>\n",
       "      <td>×</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>○</td>\n",
       "      <td>×</td>\n",
       "      <td>○</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1  2\n",
       "0  ○     ×\n",
       "1     ○  ×\n",
       "2  ○  ×  ○"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'○ wins'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run():\n",
    "    handle = display(None, display_id=True)\n",
    "    board = torch.zeros((3,3), dtype=torch.float32, device=DEVICE)\n",
    "    state = auto_play(board, policy_random, display_handle=handle)\n",
    "    msg = {-1: '× wins', 0: 'draw', 1: '○ wins'}\n",
    "    return msg[int(state)]\n",
    "\n",
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1554c3e4-3c5d-4ef2-904e-86bb112fa95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_play(board, policy, N=100):\n",
    "    \"\"\" repeat games N times\n",
    "    Return\n",
    "    counts : [count wins of later, count of draws, count wins of former]\n",
    "    \"\"\"\n",
    "    states = [int(auto_play(board, policy)) for i in range(N)]\n",
    "    u, c = np.unique(states, return_counts=True)\n",
    "    assert set(u).issubset({-1, 0, 1})\n",
    "    counts = np.zeros(3, 'i')\n",
    "    counts [u+1] = c\n",
    "    return counts\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8dfc4aa9-d31c-409b-8057-943c2d81fc4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[later draw former]\n",
      "[0.29 0.12 0.59] = [293 119 588] / 1000\n"
     ]
    }
   ],
   "source": [
    "def run(N=100):\n",
    "    board = torch.zeros((3,3), dtype=torch.float32, device=DEVICE)\n",
    "    counts = repeat_play(board, policy_random, N=N)\n",
    "    with np.printoptions(precision=2, floatmode='fixed'):\n",
    "        print('[later draw former]')\n",
    "        print(counts / counts.sum(), '=', counts, '/', counts.sum())\n",
    "\n",
    "run(N=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "91bf2ac7-092e-41cf-bde0-91ec45c5cbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate(net, board, player):\n",
    "    \"\"\" AI estimate \"\"\"\n",
    "    board = player * board\n",
    "    board = board.view(1,1, *board.size())\n",
    "    return net(board).flatten()\n",
    "\n",
    "def policy_ai(board, player, net=None):\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        blanks, = torch.where(board.flatten() == 0)\n",
    "        Q = estimate(net, board, player)\n",
    "        action = blanks[Q[blanks].argmax()]\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8beeaa85-3134-452b-8926-8ec643279706",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_train(board, player, net=None, optimizer=None,epsilon=0.5,gamma=1., report=None):\n",
    "    \"\"\" train AI \"\"\"\n",
    "    ######################\n",
    "    # estimated number of action values\n",
    "    ######################\n",
    "    net.train()\n",
    "    optimizer.zero_grad()\n",
    "    Q = estimate(net, board, player) # estimate action value\n",
    "    blanks, = torch.where(board.flatten() == 0) # get blank cell\n",
    "    if torch.rand((1,)) < epsilon:\n",
    "        i = torch.randint(len(blanks), (1,)) # select random move from blanks\n",
    "        action = blanks[i.item()]\n",
    "    else:\n",
    "        action = blanks[Q[blanks].argmax()] # select next move from the maximum value from blanks\n",
    "\n",
    "    Q_action = Q[action] # value of next action\n",
    "\n",
    "    ######################\n",
    "    # target number of action values\n",
    "    ######################\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        post = board.detach().clone()\n",
    "        state = next_move(board, player, action, out=post) # judege win/lose of next move\n",
    "        blanks_next, = torch.where(post.flatten() == 0) # get blank cells of next move\n",
    "        reward, Q_next = 0, 0\n",
    "        if state == player: # player wins\n",
    "            reward = 1 \n",
    "        elif state == -player: # player loses\n",
    "            reward = -1\n",
    "        elif len(blanks_next) == 0: # draw\n",
    "            reward = 0\n",
    "        else:                       # win/lose is not decided\n",
    "            Q = estimate(net, post, -player) # estimate action value of next of next move \n",
    "            Q_next = Q[blanks_next].max()    # select maximum value of action\n",
    "\n",
    "        Q_target = reward - gamma * Q_next\n",
    "        Q_target = torch.as_tensor(Q_target, dtype=torch.float32, device=Q_action.device)\n",
    "\n",
    "    ######################\n",
    "    # update paramaters\n",
    "    ######################\n",
    "    net.train()\n",
    "    loss = F.mse_loss(Q_action, Q_target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if report is not None:\n",
    "        report['loss'] += loss.item()\n",
    "\n",
    "    return action\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "31bd28d1-e9f4-4632-87cc-ab1ebce84693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#[later draw former]\n",
      "[1/10000] loss:1.175 %Train:[00 00 100] %1st:[35 11 54] %2nd:[25 02 73] 0.368s\n",
      "[2/10000] loss:1.187 %Train:[00 00 100] %1st:[49 07 44] %2nd:[25 02 73] 0.393s\n",
      "[3/10000] loss:1.283 %Train:[00 00 100] %1st:[51 05 44] %2nd:[34 11 55] 0.339s\n",
      "[8/10000] loss:4.420 %Train:[20 20 60] %1st:[46 05 49] %2nd:[27 05 68] 0.458s\n",
      "[9/10000] loss:1.073 %Train:[100 00 00] %1st:[36 09 55] %2nd:[34 08 58] 0.434s\n",
      "[10/10000] loss:1.132 %Train:[00 00 100] %1st:[36 07 57] %2nd:[40 07 53] 0.310s\n",
      "[11/10000] loss:0.082 %Train:[00 100 00] %1st:[43 06 51] %2nd:[36 09 55] 0.377s\n",
      "[512/10000] loss:464.851 %Train:[33 02 65] %1st:[03 00 97] %2nd:[68 01 31] 4.238s\n",
      "[513/10000] loss:0.641 %Train:[00 00 100] %1st:[05 00 95] %2nd:[70 01 29] 0.361s\n",
      "[514/10000] loss:0.172 %Train:[00 100 00] %1st:[05 00 95] %2nd:[64 02 34] 0.342s\n",
      "[515/10000] loss:1.124 %Train:[00 00 100] %1st:[08 00 92] %2nd:[77 01 22] 0.256s\n",
      "[520/10000] loss:3.856 %Train:[20 00 80] %1st:[06 02 92] %2nd:[66 01 33] 0.355s\n",
      "[521/10000] loss:0.514 %Train:[100 00 00] %1st:[07 01 92] %2nd:[77 00 23] 0.368s\n",
      "[522/10000] loss:0.750 %Train:[00 00 100] %1st:[07 04 89] %2nd:[73 00 27] 0.393s\n",
      "[523/10000] loss:1.085 %Train:[00 00 100] %1st:[03 00 97] %2nd:[76 00 24] 0.271s\n",
      "[1024/10000] loss:356.178 %Train:[32 05 62] %1st:[01 02 97] %2nd:[82 04 14] 4.144s\n",
      "[1025/10000] loss:0.164 %Train:[00 00 100] %1st:[00 02 98] %2nd:[77 05 18] 0.270s\n",
      "[1026/10000] loss:1.164 %Train:[100 00 00] %1st:[01 01 98] %2nd:[81 03 16] 0.387s\n",
      "[1027/10000] loss:0.105 %Train:[100 00 00] %1st:[01 00 99] %2nd:[81 02 17] 0.259s\n",
      "[1032/10000] loss:4.117 %Train:[20 00 80] %1st:[10 00 90] %2nd:[80 00 20] 0.349s\n",
      "[1033/10000] loss:1.065 %Train:[00 00 100] %1st:[03 00 97] %2nd:[81 00 19] 0.260s\n",
      "[1034/10000] loss:0.212 %Train:[00 00 100] %1st:[00 00 100] %2nd:[84 01 15] 0.304s\n",
      "[1035/10000] loss:1.682 %Train:[00 00 100] %1st:[02 00 98] %2nd:[84 01 15] 0.289s\n",
      "[1536/10000] loss:359.374 %Train:[27 04 69] %1st:[01 00 99] %2nd:[90 00 10] 3.396s\n",
      "[1537/10000] loss:0.162 %Train:[00 00 100] %1st:[02 00 98] %2nd:[87 00 13] 0.247s\n",
      "[1538/10000] loss:0.680 %Train:[100 00 00] %1st:[03 01 96] %2nd:[88 00 12] 0.277s\n",
      "[1539/10000] loss:0.176 %Train:[00 00 100] %1st:[00 00 100] %2nd:[84 01 15] 0.267s\n",
      "[1544/10000] loss:1.953 %Train:[40 20 40] %1st:[00 00 100] %2nd:[90 01 09] 0.294s\n",
      "[1545/10000] loss:2.146 %Train:[00 00 100] %1st:[03 01 96] %2nd:[86 05 09] 0.257s\n",
      "[1546/10000] loss:0.635 %Train:[00 00 100] %1st:[06 01 93] %2nd:[87 01 12] 0.269s\n",
      "[1547/10000] loss:0.450 %Train:[100 00 00] %1st:[07 00 93] %2nd:[84 07 09] 0.263s\n",
      "[2048/10000] loss:341.746 %Train:[25 05 70] %1st:[00 01 99] %2nd:[90 03 07] 3.361s\n",
      "[2049/10000] loss:0.336 %Train:[00 00 100] %1st:[00 01 99] %2nd:[87 02 11] 0.262s\n",
      "[2050/10000] loss:0.214 %Train:[00 00 100] %1st:[00 03 97] %2nd:[83 04 13] 0.254s\n",
      "[2051/10000] loss:0.452 %Train:[00 00 100] %1st:[00 00 100] %2nd:[80 06 14] 0.278s\n",
      "[2056/10000] loss:3.007 %Train:[20 00 80] %1st:[00 01 99] %2nd:[90 02 08] 0.297s\n",
      "[2057/10000] loss:0.167 %Train:[00 00 100] %1st:[00 01 99] %2nd:[89 03 08] 0.270s\n",
      "[2058/10000] loss:0.526 %Train:[100 00 00] %1st:[00 00 100] %2nd:[89 00 11] 0.261s\n",
      "[2059/10000] loss:0.999 %Train:[00 00 100] %1st:[00 01 99] %2nd:[89 02 09] 0.278s\n",
      "[2560/10000] loss:310.484 %Train:[32 09 59] %1st:[01 02 97] %2nd:[90 04 06] 3.457s\n",
      "[2561/10000] loss:0.527 %Train:[100 00 00] %1st:[00 01 99] %2nd:[93 02 05] 0.269s\n",
      "[2562/10000] loss:0.614 %Train:[100 00 00] %1st:[00 01 99] %2nd:[90 05 05] 0.272s\n",
      "[2563/10000] loss:0.348 %Train:[100 00 00] %1st:[01 06 93] %2nd:[86 09 05] 0.277s\n",
      "[2568/10000] loss:2.954 %Train:[00 00 100] %1st:[00 03 97] %2nd:[91 05 04] 0.291s\n",
      "[2569/10000] loss:0.054 %Train:[00 00 100] %1st:[00 01 99] %2nd:[92 07 01] 0.273s\n",
      "[2570/10000] loss:0.389 %Train:[00 00 100] %1st:[00 03 97] %2nd:[94 05 01] 0.347s\n",
      "[2571/10000] loss:0.065 %Train:[00 00 100] %1st:[00 03 97] %2nd:[89 06 05] 0.305s\n",
      "[3072/10000] loss:280.555 %Train:[25 14 61] %1st:[00 00 100] %2nd:[94 04 02] 3.772s\n",
      "[3073/10000] loss:0.113 %Train:[00 00 100] %1st:[00 03 97] %2nd:[92 06 02] 0.279s\n",
      "[3074/10000] loss:2.211 %Train:[00 00 100] %1st:[00 00 100] %2nd:[91 05 04] 0.281s\n",
      "[3075/10000] loss:0.601 %Train:[00 00 100] %1st:[00 00 100] %2nd:[93 04 03] 0.290s\n",
      "[3080/10000] loss:4.458 %Train:[80 00 20] %1st:[00 04 96] %2nd:[93 05 02] 0.342s\n",
      "[3081/10000] loss:0.642 %Train:[00 00 100] %1st:[00 00 100] %2nd:[89 06 05] 0.282s\n",
      "[3082/10000] loss:0.098 %Train:[00 00 100] %1st:[00 01 99] %2nd:[88 09 03] 0.312s\n",
      "[3083/10000] loss:0.102 %Train:[100 00 00] %1st:[00 01 99] %2nd:[94 05 01] 0.326s\n",
      "[3584/10000] loss:257.320 %Train:[28 14 58] %1st:[00 01 99] %2nd:[89 08 03] 4.377s\n",
      "[3585/10000] loss:0.909 %Train:[00 100 00] %1st:[00 00 100] %2nd:[92 04 04] 0.287s\n",
      "[3586/10000] loss:0.991 %Train:[00 100 00] %1st:[00 03 97] %2nd:[89 07 04] 0.314s\n",
      "[3587/10000] loss:0.338 %Train:[100 00 00] %1st:[00 05 95] %2nd:[85 12 03] 0.359s\n",
      "[3592/10000] loss:4.355 %Train:[20 40 40] %1st:[00 01 99] %2nd:[83 14 03] 0.309s\n",
      "[3593/10000] loss:0.520 %Train:[00 00 100] %1st:[00 00 100] %2nd:[93 04 03] 0.309s\n",
      "[3594/10000] loss:0.426 %Train:[100 00 00] %1st:[00 02 98] %2nd:[91 04 05] 0.275s\n",
      "[3595/10000] loss:0.220 %Train:[00 00 100] %1st:[00 02 98] %2nd:[93 06 01] 0.339s\n",
      "[4096/10000] loss:222.173 %Train:[26 19 55] %1st:[00 02 98] %2nd:[91 07 02] 3.976s\n",
      "[4097/10000] loss:0.583 %Train:[100 00 00] %1st:[00 03 97] %2nd:[85 15 00] 0.318s\n",
      "[4098/10000] loss:0.367 %Train:[00 00 100] %1st:[00 02 98] %2nd:[83 11 06] 0.290s\n",
      "[4099/10000] loss:0.551 %Train:[00 00 100] %1st:[00 02 98] %2nd:[89 10 01] 0.298s\n",
      "[4104/10000] loss:2.459 %Train:[00 60 40] %1st:[00 03 97] %2nd:[90 08 02] 0.341s\n",
      "[4105/10000] loss:0.443 %Train:[00 00 100] %1st:[00 05 95] %2nd:[89 07 04] 0.289s\n",
      "[4106/10000] loss:0.072 %Train:[100 00 00] %1st:[00 03 97] %2nd:[87 12 01] 0.274s\n",
      "[4107/10000] loss:0.866 %Train:[00 100 00] %1st:[00 02 98] %2nd:[91 07 02] 0.278s\n",
      "[4608/10000] loss:210.822 %Train:[23 19 57] %1st:[00 01 99] %2nd:[88 08 04] 3.877s\n",
      "[4609/10000] loss:0.428 %Train:[00 00 100] %1st:[00 00 100] %2nd:[87 12 01] 0.308s\n",
      "[4610/10000] loss:0.625 %Train:[00 100 00] %1st:[00 02 98] %2nd:[88 12 00] 0.277s\n",
      "[4611/10000] loss:0.480 %Train:[00 00 100] %1st:[00 00 100] %2nd:[91 09 00] 0.291s\n",
      "[4616/10000] loss:1.097 %Train:[20 40 40] %1st:[00 02 98] %2nd:[94 06 00] 0.350s\n",
      "[4617/10000] loss:0.192 %Train:[00 100 00] %1st:[00 01 99] %2nd:[95 04 01] 0.281s\n",
      "[4618/10000] loss:0.474 %Train:[00 00 100] %1st:[00 01 99] %2nd:[90 08 02] 0.287s\n",
      "[4619/10000] loss:0.233 %Train:[00 00 100] %1st:[00 00 100] %2nd:[88 10 02] 0.298s\n",
      "[5120/10000] loss:187.875 %Train:[24 20 56] %1st:[00 01 99] %2nd:[90 09 01] 3.791s\n",
      "[5121/10000] loss:0.231 %Train:[00 100 00] %1st:[00 00 100] %2nd:[89 10 01] 0.284s\n",
      "[5122/10000] loss:0.790 %Train:[00 100 00] %1st:[00 00 100] %2nd:[86 14 00] 0.293s\n",
      "[5123/10000] loss:0.076 %Train:[00 100 00] %1st:[00 03 97] %2nd:[85 14 01] 0.292s\n",
      "[5128/10000] loss:0.757 %Train:[20 00 80] %1st:[00 02 98] %2nd:[92 07 01] 0.380s\n",
      "[5129/10000] loss:0.726 %Train:[00 100 00] %1st:[00 03 97] %2nd:[94 05 01] 0.440s\n",
      "[5130/10000] loss:0.855 %Train:[00 100 00] %1st:[00 01 99] %2nd:[90 09 01] 0.368s\n",
      "[5131/10000] loss:0.074 %Train:[00 00 100] %1st:[00 02 98] %2nd:[87 12 01] 0.348s\n",
      "[5632/10000] loss:184.799 %Train:[24 24 51] %1st:[00 02 98] %2nd:[92 06 02] 4.555s\n",
      "[5633/10000] loss:0.102 %Train:[00 00 100] %1st:[00 03 97] %2nd:[91 09 00] 0.383s\n",
      "[5634/10000] loss:0.678 %Train:[00 00 100] %1st:[00 00 100] %2nd:[90 10 00] 0.402s\n",
      "[5635/10000] loss:0.700 %Train:[00 100 00] %1st:[00 01 99] %2nd:[89 10 01] 0.349s\n",
      "[5640/10000] loss:1.735 %Train:[00 60 40] %1st:[00 00 100] %2nd:[95 04 01] 0.325s\n",
      "[5641/10000] loss:0.255 %Train:[100 00 00] %1st:[00 01 99] %2nd:[94 05 01] 0.299s\n",
      "[5642/10000] loss:0.839 %Train:[100 00 00] %1st:[00 01 99] %2nd:[92 08 00] 0.322s\n",
      "[5643/10000] loss:0.240 %Train:[00 00 100] %1st:[00 02 98] %2nd:[86 13 01] 0.357s\n",
      "[6144/10000] loss:163.951 %Train:[23 20 56] %1st:[00 02 98] %2nd:[88 11 01] 4.235s\n",
      "[6145/10000] loss:0.395 %Train:[100 00 00] %1st:[00 01 99] %2nd:[89 11 00] 0.274s\n",
      "[6146/10000] loss:0.708 %Train:[00 100 00] %1st:[00 03 97] %2nd:[89 10 01] 0.392s\n",
      "[6147/10000] loss:0.174 %Train:[100 00 00] %1st:[00 01 99] %2nd:[90 09 01] 0.273s\n",
      "[6152/10000] loss:1.560 %Train:[00 00 100] %1st:[00 00 100] %2nd:[86 14 00] 0.320s\n",
      "[6153/10000] loss:0.127 %Train:[00 00 100] %1st:[00 03 97] %2nd:[90 09 01] 0.280s\n",
      "[6154/10000] loss:0.090 %Train:[00 00 100] %1st:[00 00 100] %2nd:[86 14 00] 0.281s\n",
      "[6155/10000] loss:0.285 %Train:[00 00 100] %1st:[00 01 99] %2nd:[86 14 00] 0.284s\n",
      "[6656/10000] loss:170.758 %Train:[25 22 53] %1st:[00 01 99] %2nd:[87 12 01] 4.082s\n",
      "[6657/10000] loss:0.229 %Train:[00 100 00] %1st:[00 02 98] %2nd:[93 06 01] 0.398s\n",
      "[6658/10000] loss:0.382 %Train:[00 100 00] %1st:[00 00 100] %2nd:[90 09 01] 0.301s\n",
      "[6659/10000] loss:0.034 %Train:[00 00 100] %1st:[00 02 98] %2nd:[87 12 01] 0.406s\n",
      "[6664/10000] loss:1.462 %Train:[20 40 40] %1st:[00 01 99] %2nd:[89 09 02] 0.378s\n",
      "[6665/10000] loss:0.182 %Train:[00 00 100] %1st:[00 03 97] %2nd:[87 12 01] 0.423s\n",
      "[6666/10000] loss:0.344 %Train:[100 00 00] %1st:[00 00 100] %2nd:[90 10 00] 0.363s\n",
      "[6667/10000] loss:0.158 %Train:[00 00 100] %1st:[00 01 99] %2nd:[87 13 00] 0.400s\n",
      "[7168/10000] loss:180.215 %Train:[25 24 51] %1st:[00 02 98] %2nd:[94 06 00] 4.257s\n",
      "[7169/10000] loss:0.289 %Train:[00 100 00] %1st:[00 01 99] %2nd:[89 11 00] 0.373s\n",
      "[7170/10000] loss:1.144 %Train:[00 100 00] %1st:[00 00 100] %2nd:[86 14 00] 0.317s\n",
      "[7171/10000] loss:0.185 %Train:[00 00 100] %1st:[00 01 99] %2nd:[89 11 00] 0.426s\n",
      "[7176/10000] loss:2.281 %Train:[00 00 100] %1st:[00 05 95] %2nd:[89 10 01] 0.333s\n",
      "[7177/10000] loss:0.163 %Train:[00 00 100] %1st:[00 03 97] %2nd:[83 16 01] 0.396s\n",
      "[7178/10000] loss:0.651 %Train:[00 100 00] %1st:[00 02 98] %2nd:[87 13 00] 0.301s\n",
      "[7179/10000] loss:0.308 %Train:[100 00 00] %1st:[00 03 97] %2nd:[88 11 01] 0.378s\n",
      "[7680/10000] loss:162.307 %Train:[23 23 54] %1st:[00 04 96] %2nd:[88 12 00] 4.231s\n",
      "[7681/10000] loss:0.034 %Train:[00 00 100] %1st:[00 03 97] %2nd:[91 09 00] 0.314s\n",
      "[7682/10000] loss:0.389 %Train:[00 00 100] %1st:[00 00 100] %2nd:[89 11 00] 0.306s\n",
      "[7683/10000] loss:0.112 %Train:[00 00 100] %1st:[00 03 97] %2nd:[92 07 01] 0.286s\n",
      "[7688/10000] loss:0.680 %Train:[20 20 60] %1st:[00 01 99] %2nd:[89 11 00] 0.308s\n",
      "[7689/10000] loss:0.015 %Train:[00 00 100] %1st:[00 05 95] %2nd:[92 08 00] 0.307s\n",
      "[7690/10000] loss:0.662 %Train:[00 00 100] %1st:[00 03 97] %2nd:[89 07 04] 0.298s\n",
      "[7691/10000] loss:0.236 %Train:[00 00 100] %1st:[00 00 100] %2nd:[87 13 00] 0.270s\n",
      "[8192/10000] loss:145.875 %Train:[22 27 51] %1st:[00 02 98] %2nd:[91 09 00] 4.399s\n",
      "[8193/10000] loss:0.533 %Train:[00 00 100] %1st:[00 01 99] %2nd:[86 13 01] 0.320s\n",
      "[8194/10000] loss:0.179 %Train:[100 00 00] %1st:[00 01 99] %2nd:[95 05 00] 0.323s\n",
      "[8195/10000] loss:0.543 %Train:[00 00 100] %1st:[00 01 99] %2nd:[85 15 00] 0.323s\n",
      "[8200/10000] loss:1.824 %Train:[00 40 60] %1st:[00 01 99] %2nd:[94 06 00] 0.335s\n",
      "[8201/10000] loss:0.156 %Train:[00 00 100] %1st:[00 02 98] %2nd:[89 11 00] 0.317s\n",
      "[8202/10000] loss:0.109 %Train:[00 00 100] %1st:[00 02 98] %2nd:[90 10 00] 0.364s\n",
      "[8203/10000] loss:0.170 %Train:[00 00 100] %1st:[00 00 100] %2nd:[90 10 00] 0.352s\n",
      "[8704/10000] loss:149.372 %Train:[24 24 52] %1st:[00 02 98] %2nd:[95 03 02] 4.964s\n",
      "[8705/10000] loss:0.395 %Train:[00 00 100] %1st:[00 00 100] %2nd:[92 07 01] 0.337s\n",
      "[8706/10000] loss:0.126 %Train:[100 00 00] %1st:[00 02 98] %2nd:[93 06 01] 0.328s\n",
      "[8707/10000] loss:0.182 %Train:[00 00 100] %1st:[00 03 97] %2nd:[85 13 02] 0.321s\n",
      "[8712/10000] loss:0.968 %Train:[20 40 40] %1st:[00 01 99] %2nd:[93 07 00] 0.323s\n",
      "[8713/10000] loss:0.125 %Train:[100 00 00] %1st:[00 01 99] %2nd:[88 10 02] 0.368s\n",
      "[8714/10000] loss:0.409 %Train:[00 00 100] %1st:[00 01 99] %2nd:[88 12 00] 0.484s\n",
      "[8715/10000] loss:0.038 %Train:[00 00 100] %1st:[00 02 98] %2nd:[83 15 02] 0.375s\n",
      "[9216/10000] loss:146.630 %Train:[26 22 52] %1st:[00 01 99] %2nd:[93 07 00] 4.849s\n",
      "[9217/10000] loss:0.036 %Train:[00 00 100] %1st:[00 00 100] %2nd:[93 06 01] 0.320s\n",
      "[9218/10000] loss:0.164 %Train:[100 00 00] %1st:[00 02 98] %2nd:[91 08 01] 0.373s\n",
      "[9219/10000] loss:0.052 %Train:[00 00 100] %1st:[00 03 97] %2nd:[92 08 00] 0.455s\n",
      "[9224/10000] loss:2.044 %Train:[20 20 60] %1st:[00 03 97] %2nd:[89 11 00] 0.380s\n",
      "[9225/10000] loss:0.242 %Train:[00 100 00] %1st:[00 03 97] %2nd:[92 08 00] 0.439s\n",
      "[9226/10000] loss:0.791 %Train:[00 00 100] %1st:[00 02 98] %2nd:[92 07 01] 0.311s\n",
      "[9227/10000] loss:0.271 %Train:[00 100 00] %1st:[00 03 97] %2nd:[84 15 01] 0.354s\n",
      "[9728/10000] loss:111.000 %Train:[20 24 56] %1st:[00 01 99] %2nd:[88 12 00] 4.613s\n",
      "[9729/10000] loss:0.211 %Train:[00 100 00] %1st:[00 01 99] %2nd:[91 09 00] 0.339s\n",
      "[9730/10000] loss:0.262 %Train:[00 00 100] %1st:[00 01 99] %2nd:[83 17 00] 0.293s\n",
      "[9731/10000] loss:0.098 %Train:[00 00 100] %1st:[00 01 99] %2nd:[89 11 00] 0.325s\n",
      "[9736/10000] loss:1.229 %Train:[40 20 40] %1st:[00 00 100] %2nd:[85 15 00] 0.363s\n",
      "[9737/10000] loss:0.030 %Train:[100 00 00] %1st:[00 02 98] %2nd:[83 17 00] 0.313s\n",
      "[9738/10000] loss:0.083 %Train:[00 00 100] %1st:[00 02 98] %2nd:[87 13 00] 0.424s\n",
      "[9739/10000] loss:0.161 %Train:[00 00 100] %1st:[00 05 95] %2nd:[86 14 00] 0.285s\n",
      "[10000/10000] loss:61.669 %Train:[27 26 46] %1st:[00 01 99] %2nd:[84 16 00] 2.810s\n"
     ]
    }
   ],
   "source": [
    "def run(n_episode=10000, interval=500, lr=0.01, N=100):\n",
    "    dim = 128\n",
    "    cnn = nn.Sequential(\n",
    "        nn.Conv2d(1, dim, kernel_size=3, padding=0, bias=False),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(dim, dim, bias=True),\n",
    "        nn.ReLU(True),\n",
    "        nn.Linear(dim, dim, bias=True),\n",
    "        nn.ReLU(True),\n",
    "        nn.Linear(dim, 3*3, bias=True),\n",
    "        nn.Tanh(),\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    # policies while evaluation\n",
    "    ai_vs_random = (\n",
    "        functools.partial(policy_ai, net=cnn),\n",
    "        policy_random,\n",
    "    )\n",
    "    random_vs_ai = (\n",
    "        policy_random,\n",
    "        functools.partial(policy_ai, net=cnn),\n",
    "    )\n",
    "\n",
    "    # policies while learning\n",
    "    op = torch.optim.SGD(cnn.parameters(), lr=lr)\n",
    "    rp = {'loss': 0.}\n",
    "    policy = functools.partial(policy_train, net=cnn, optimizer=op, report=rp)\n",
    "    \n",
    "    print('#[later draw former]')\n",
    "    n_train = np.array([0,0,0])\n",
    "    board = torch.zeros((3,3), dtype=torch.float32, device=DEVICE)\n",
    "    start_tm = time.time()\n",
    "    for i in range(n_episode):\n",
    "        winner = int(auto_play(board, policy))\n",
    "        n_train[winner+1] += 1\n",
    "\n",
    "        if i==0 or (i+1) & interval == 0 or i+1 == n_episode:\n",
    "            n_1st = repeat_play(board, ai_vs_random, N=N)\n",
    "            n_2nd = repeat_play(board, random_vs_ai, N=N)\n",
    "            loss = rp['loss']\n",
    "            current_tm =  time.time()\n",
    "\n",
    "            with np.printoptions(formatter={'float': '{:02.0f}'.format}):\n",
    "                print('[{}/{}] loss:{:.3f} %Train:{} %1st:{} %2nd:{} {:.3f}s'.format(\n",
    "                    i+1, n_episode, loss,\n",
    "                    100 * n_train / n_train.sum(),\n",
    "                    100 * n_1st / n_1st.sum(),\n",
    "                    100 * n_2nd / n_2nd.sum(),\n",
    "                    current_tm - start_tm,\n",
    "                ))\n",
    "\n",
    "            n_train[:] = 0\n",
    "            rp['loss'] = 0\n",
    "            start_tm = current_tm\n",
    "\n",
    "    return cnn\n",
    "\n",
    "cnn = run(n_episode=10000, interval=500, lr=0.01, N=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
